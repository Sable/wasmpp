{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WasmDNN WasmDNN is a deep neural network library for generating machine learning models in WebAssembly. The library is written in C++, but an alternative JavaScript/WebAssembly version is also available by compiling the project using Emscripten. Getting Started Architecture Features Demo Online version","title":"Home"},{"location":"#wasmdnn","text":"WasmDNN is a deep neural network library for generating machine learning models in WebAssembly. The library is written in C++, but an alternative JavaScript/WebAssembly version is also available by compiling the project using Emscripten.","title":"WasmDNN"},{"location":"#getting-started","text":"Architecture Features","title":"Getting Started"},{"location":"#demo","text":"Online version","title":"Demo"},{"location":"architecture/","text":"WasmDNN WasmDNN is a deep neural network library for generating machine learning models in WebAssembly. The library is written in C++, but an alternative JavaScript/WebAssembly version is also available by compiling the project using Emscripten. Architecture Pipeline Above is an overview diagram of the WasmDNN library pipeline using an example model designed to operate on the MNIST dataset. To configure this model, one can either do this in C++ or in WebAssembly using JavaScript bindings. In the C++ option, the model configuration is done using the WasmDNN library. In the JavaScript bindings, the model configuration is done using the WebAssembly version of the library compiled, together with Wasm++ and WABT, using Emscripten. Both model configuration options are equivalent and should be capable of generating identical machine learning Wasm model to train, test and predict on the MNIST dataset.","title":"Architecture"},{"location":"architecture/#wasmdnn","text":"WasmDNN is a deep neural network library for generating machine learning models in WebAssembly. The library is written in C++, but an alternative JavaScript/WebAssembly version is also available by compiling the project using Emscripten.","title":"WasmDNN"},{"location":"architecture/#architecture","text":"","title":"Architecture"},{"location":"architecture/#pipeline","text":"Above is an overview diagram of the WasmDNN library pipeline using an example model designed to operate on the MNIST dataset. To configure this model, one can either do this in C++ or in WebAssembly using JavaScript bindings. In the C++ option, the model configuration is done using the WasmDNN library. In the JavaScript bindings, the model configuration is done using the WebAssembly version of the library compiled, together with Wasm++ and WABT, using Emscripten. Both model configuration options are equivalent and should be capable of generating identical machine learning Wasm model to train, test and predict on the MNIST dataset.","title":"Pipeline"},{"location":"features/","text":"Features Activation Functions Activation function Layer usage JavaScript C++ Sigmoid Hidden and Output sigmoid Sigmoid() Softmax Output softmax Softmax() ReLU Hidden relu ReLU() LeakyReLU Hidden leakyrelu LeakyReLU() ELU Hidden elu ELU() Tanh Hidden tanh Tanh() Loss Functions Loss function Output layer activation function JavaScript C++ Mean Squared Error Sigmoid mean-squared-error MeanSquaredError() Sigmoid Cross Entropy Sigmoid sigmoid-cross-entropy SigmoidCrossEntropy() Softmax Cross Entropy Softmax softmax-cross-entropy SoftmaxCrossEntropy() Weight Initialization Weight initializer Layer Position JavaScript C++ Xavier Uniform Hidden xavier_uniform XavierUniform() Xavier Normal Hidden xavier_normal XavierNormal() LeCun Uniform Hidden and Output lecun_uniform LeCunUniform() LeCun Normal Hidden and Output lecun_normal LeCunNormal() Gaussian Hidden and Output gaussian Gaussian() Uniform Hidden and Output uniform Uniform() Constant Hidden and Output constant Constant() Weights Optimizer Weights Optimizer Stochastic Gradient Descent Regularization Regularization Scope Dropout Input layer and Hidden layers L1 All weights and bias values L2 All weights and bias values","title":"Features"},{"location":"features/#features","text":"","title":"Features"},{"location":"features/#activation-functions","text":"Activation function Layer usage JavaScript C++ Sigmoid Hidden and Output sigmoid Sigmoid() Softmax Output softmax Softmax() ReLU Hidden relu ReLU() LeakyReLU Hidden leakyrelu LeakyReLU() ELU Hidden elu ELU() Tanh Hidden tanh Tanh()","title":"Activation Functions"},{"location":"features/#loss-functions","text":"Loss function Output layer activation function JavaScript C++ Mean Squared Error Sigmoid mean-squared-error MeanSquaredError() Sigmoid Cross Entropy Sigmoid sigmoid-cross-entropy SigmoidCrossEntropy() Softmax Cross Entropy Softmax softmax-cross-entropy SoftmaxCrossEntropy()","title":"Loss Functions"},{"location":"features/#weight-initialization","text":"Weight initializer Layer Position JavaScript C++ Xavier Uniform Hidden xavier_uniform XavierUniform() Xavier Normal Hidden xavier_normal XavierNormal() LeCun Uniform Hidden and Output lecun_uniform LeCunUniform() LeCun Normal Hidden and Output lecun_normal LeCunNormal() Gaussian Hidden and Output gaussian Gaussian() Uniform Hidden and Output uniform Uniform() Constant Hidden and Output constant Constant()","title":"Weight Initialization"},{"location":"features/#weights-optimizer","text":"Weights Optimizer Stochastic Gradient Descent","title":"Weights Optimizer"},{"location":"features/#regularization","text":"Regularization Scope Dropout Input layer and Hidden layers L1 All weights and bias values L2 All weights and bias values","title":"Regularization"}]}